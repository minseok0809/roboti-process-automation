{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook NLLB Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Development Envrionment\n",
    "<br>Data Exploration\n",
    "<br>NHNDQ/nllb-finetuned-en2ko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development Envrionment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "# import jsonlines\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"lmqg/qg_squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['answer', 'paragraph_question', 'question', 'sentence', 'paragraph', 'sentence_answer', 'paragraph_answer', 'paragraph_sentence'],\n",
       "        num_rows: 75722\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['answer', 'paragraph_question', 'question', 'sentence', 'paragraph', 'sentence_answer', 'paragraph_answer', 'paragraph_sentence'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['answer', 'paragraph_question', 'question', 'sentence', 'paragraph', 'sentence_answer', 'paragraph_answer', 'paragraph_sentence'],\n",
       "        num_rows: 11877\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_data = \"sample/train00.jsonl\"\n",
    "with open(jsonl_data) as f:\n",
    "    jsonl_df = pd.DataFrame(json.loads(line) for line in f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_jsonl_df = jsonl_df[['paragraph_answer', 'question']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('sample/test_train00.jsonl', 'w')\n",
    "print(sample_jsonl_df.to_json(orient='records', lines=True),file=f, flush=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 0)\n",
    "pd.options.display.max_rows = None\n",
    "jsonl_df[['paragraph_answer', 'question']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NHNDQ/nllb-finetuned-en2ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_list(path_list):\n",
    "    \n",
    "    path_list = sorted(path_list, reverse=False)\n",
    "    path_list = sorted(path_list, key=len)\n",
    "    \n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline \n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "import datasets \n",
    "datasets.disable_progress_bar()\n",
    "\n",
    "device_num = 1\n",
    "translator = pipeline('translation', model='NHNDQ/nllb-finetuned-en2ko', #Model finetuned from facebook/nllb-200-distilled-600M]\n",
    "                      device=device_num, src_lang='eng_Latn', tgt_lang='kor_Hang', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "록히드마틴이 미국 해병대에 최초 5G 테스트베드를 전달하고 모바일 네트워크 실험에 나선다.\n"
     ]
    }
   ],
   "source": [
    "text = 'Lockheed Martin Delivers Initial 5G Testbed To U.S. Marine Corps And Begins Mobile Network Experimentation'\n",
    "output = translator(text, max_length=512)\n",
    "print(output[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of jsonl file: 32\n",
      "\n",
      "0 dev00\n",
      "1 dev01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaconda3/lib/python3.9/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 dev02\n",
      "3 dev03\n",
      "4 test00\n",
      "5 test01\n",
      "6 test02\n",
      "7 test03\n",
      "8 train00\n",
      "9 train01\n",
      "10 train02\n",
      "11 train03\n",
      "12 train04\n",
      "13 train05\n",
      "14 train06\n",
      "15 train07\n",
      "16 train08\n",
      "17 train09\n",
      "18 train10\n",
      "19 train11\n",
      "20 train12\n",
      "21 train13\n",
      "22 train14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 955 is bigger than 0.9 * max_length: 1000. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 train15\n",
      "24 train16\n",
      "25 train17\n",
      "26 train18\n",
      "27 train19\n",
      "28 train20\n",
      "29 train21\n",
      "30 train22\n",
      "31 test_train00\n"
     ]
    }
   ],
   "source": [
    "jsonl_paths = glob.glob(\"./sample/*.jsonl\")\n",
    "jsonl_paths = sorted_list(jsonl_paths)\n",
    "print(\"The number of jsonl file:\", len(jsonl_paths))\n",
    "print()\n",
    "\n",
    "for idx, jsonl_path in enumerate(jsonl_paths):\n",
    "    \n",
    "    save_path = jsonl_path.replace(\"sample\", \"translation\").replace(\"jsonl\", \"xlsx\")\n",
    "    data_file = jsonl_path.split(\"/\")[-1]\n",
    "    data_file = data_file.split(\".\")[0]\n",
    "\n",
    "    if idx % 4 == 0 and idx != 0:\n",
    "        print()\n",
    "        \n",
    "    print(idx, data_file, end=\"  |  \")\n",
    "    \n",
    "    paragraph_answer_translation = []\n",
    "    question_translation = []\n",
    "    dataset = load_dataset(\"json\", data_files=jsonl_path)\n",
    "    \n",
    "    result_j = translator(KeyDataset(dataset['train'], 'paragraph_answer'), max_length=1000, batch_size=64)\n",
    "    for idx, extracted_entities in enumerate(result_j):\n",
    "        for entity in extracted_entities:\n",
    "            paragraph_answer_translation.append(entity['translation_text'])\n",
    "\n",
    "    result_k = translator(KeyDataset(dataset['train'], 'question'), max_length=1000, batch_size=64)\n",
    "    for idx, extracted_entities in enumerate(result_k):\n",
    "        for entity in extracted_entities:\n",
    "            question_translation.append(entity['translation_text'])\n",
    "\n",
    "    translation_df = pd.DataFrame({'paragraph_answer':paragraph_answer_translation,\n",
    "                                    'question':question_translation})\n",
    "    \n",
    "    translation_df.to_excel(save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>HuggingFace</b>\n",
    "<br>[lmqg/qg_squad](https://huggingface.co/datasets/lmqg/qg_squad)\n",
    "<br>[NHNDQ/nllb-finetuned-en2ko](https://huggingface.co/NHNDQ/nllb-finetuned-en2ko)\n",
    "<br>[facebook/nllb-200-distilled-600M](https://huggingface.co/facebook/nllb-200-distilled-600M)\n",
    "\n",
    "<br><b>Blog</b>\n",
    "<br>[Meta에 항상 감사하십시오. NLLB-200 모델을 이용한 기계번역](https://int-i.github.io/python/2023-09-05/nllb-en-ko-translation/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
